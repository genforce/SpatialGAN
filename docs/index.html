<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>SpacialGAN</title>

  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->


<body>


<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
  <div class="header">
    <div class="logo">
      <a href="https://genforce.github.io/" target="_blank"><img src="./assets/genforce.png"></a>
    </div>
    <div class="title", style="padding-top: 25;">  <!-- Set padding as 10 if title is with two lines. -->
      Spatial Steerability of GANs via <br /> Self-Supervision from Discriminator
    </div>
  </div>
  <!-- === Title Ends === -->
  <div class="author">
    <a href="https://jytime.github.io/" target="_blank">Jianyuan Wang</a><sup>1,2</sup>,&nbsp;
    <a href="https://lalitbhagat7.github.io/" target="_blank">Lalit Bhagat</a><sup>3</sup>,&nbsp;
    <a href="http://ceyuan.me/" target="_blank">Ceyuan Yang</a><sup>1</sup>,&nbsp;
    <a href="https://justimyhxu.github.io/" target="_blank">Yinghao Xu</a><sup>1</sup>,&nbsp;
    <a href="http://shenyujun.github.io/" target="_blank">Yujun Shen</a><sup>1</sup>,&nbsp;
    <br>
    <a href="http://users.cecs.anu.edu.au/~hongdong/" target="_blank">Hongdong Li</a><sup>2</sup>,&nbsp;
    <a href="https://boleizhou.github.io/" target="_blank">Bolei Zhou</a><sup>3</sup>&nbsp;
  </div>
  <div class="institution">
    <div>
      <sup>1</sup> The Chinese University of Hong Kong, <sup>2</sup> The Australian National University, <br>
      <sup>3</sup> University of California, Los Angeles
    </div>
  </div>

  <div class="link">
    <a href="https://arxiv.org/abs/2301.08455.pdf" target="_blank">[Paper]</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://lalitbhagat7.github.io/sGAN/index.html" target="_blank">[Code]</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://lalitbhagat7.github.io/sGAN/index.html" target="_blank">[Demo]</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </div>

  <div class="teaser">
    <a href="#demo"><img src="assets/UI_gradio_ver2.png" style="width: 70%;"></a><br>
    <font size="3">** Interactive editing on the output synthesis of SpacialGAN, by altering spatial heatmaps.</font>
  </div>

</div>
<!-- === Home Section Ends === -->



<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Overview</div>
  <div class="body">
    Generative models have made huge progress in photorealistic image synthesis in recent years. To enable human to steer the image generation process and customize the output, many works explore the interpretable dimensions of the latent space in GANs. Existing methods edit the attributes of the output image such as orientation or color scheme by varying the latent code along certain directions. However, these methods usually require additional human annotations for each pre-trained model, and they mostly focus on editing global attributes. 
    <br>
    <br>
    We design randomly sampled Gaussian heatmaps to be encoded into the intermediate layers of generative models as spatial inductive bias. Along with training the GAN model from scratch, these heatmaps are being aligned with the emerging attention of the GANâ€™s discriminator in a self-supervised learning manner. During inference, users can interact with the spatial heatmaps intuitively, enabling them to edit the output image by adjusting the scene layout, and moving, or removing objects.
 </div>
</div>
<!-- === Overview Section Ends === -->





<!-- === Demo Section Starts === -->


<div class="section">
  <div class="title">Demo</div>
  <div class="body">

    <p style="margin-top: 20pt"><a name="demo"></a></p>

    We build an interactive interface to visualize, SpacialGAN enables the interactive spatial editing of the output image.
    <div style="position: relative; padding-top: 50%; margin: 20pt auto; text-align: center;">
      <iframe src="https://www.youtube.com/embed/k7sG4XY5rIc" frameborder=0
              style="position: absolute; top: 2.5%; left: 2.5%; width: 95%; height: 100%;"
              allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen></iframe>
    </div>

  </div>
</div>


<!-- === Demo Section Ends === -->





<!-- === Result Section Starts === -->
<!--
<div class="section">
  <div class="title">Quantitative Results</div>
  <div class="body">

    With the help of spatial information encoding (represented by SEL) and spatial awarenss alignment (represented by the alignment loss), our method can push the two-player game of GANs toward the equilibrium and thereby improve the synthesis quality. The baseline is the state-of-the-art image synthesis method StyleGAN2. The image synthesis quality is quantified by the metric FID while the disequilibrium degree is quantified by the metric DI.
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/quan.png" width="60%"></td>
      </tr>
    </table>

  </div>
</div>
 -->
<!-- === Result Section Ends === -->



<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Qualitative Results</div>
  <div class="body">


    <!-- Adjust the number of rows and columns (EVERY project differs). -->

    Below, we present samples generated by the SpacialGAN model, showcasing manipulations of Multi-Object Indoor Scenes from the LSUN Bedroom dataset.
    <br>
    <br>
    In this example, we showcase the rearrangement of objects using sub-heatmap manipulation. The yellow arrows indicate the movement of objects like windows and beds, enhancing overall coherence.

    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/Bedroom_combine_a.png" width="95%"></td>
      </tr>
    </table>

    Then, we demonstrate the gradual removal of objects by eliminating their associated sub-heatmaps. Elements such as windows and lamps are gradually removed, while the background and other objects remain mostly unchanged.
    
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/Bedroom_combine_b.png" width="95%"></td>
      </tr>
    </table>

    Next, we explore the alteration of local regions by applying unique style codes to individual sub-heatmaps. This process enables a variety of changes, including variations in paintings, windows, and light types, as denoted by the blue and green boxes.


    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/Bedroom_combine_c.png" width="95%"></td>
      </tr>
    </table>

<!--
    We also provide the results on the FFHQ dataset and the LSUN Church dataset. Each row uses the same spatial heatmap but different latent codes, and each column uses the same latent code.

    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/ffhq.png" width="95%"></td>
      </tr>
    </table>


    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/ch.png" width="95%"></td>
      </tr>
    </table>
 -->
  </div>
</div>
<!-- === Result Section Ends === -->



<div class="section">
  <div class="title">Synergy between DragGAN and SpatialGAN</div>
  <div class="body">

    We also integrate the recent point-based manipulation technique, DragGAN, into our method. This integration combines the strengths of both approaches to achieve high-quality, fine-grained manipulation in a reasonable time. 
    
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./assets/Method_sgan_dragGAN.jpg" width="95%"></td>
      </tr>
    </table>

  </div>
</div>



<!-- === Reference Section Starts === -->
<div class="section">
  <div class="bibtex">BibTeX</div>
<pre>
@article{wang2023spatial,
  title={Spatial Steerability of GANs via Self-Supervision from Discriminator},
  author={Wang, Jianyuan and Yang, Ceyuan and Xu, Yinghao and Shen, Yujun and Li, Hongdong and Zhou, Bolei},
  journal={arXiv preprint arXiv:2301.08455},
  year={2023}
}
</pre>

  <!--
  <div class="ref">Related Work</div>
  <div class="citation">
    <div class="image"><img src="./assets/sg2.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/pdf/1912.04958.pdf" target="_blank">
        T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, T. Aila.
        Analyzing and Improving the Image Quality of StyleGAN.
        CVPR 2020.</a><br>
      <b>Comment:</b>
      Proposes a style-based generator StyleGAN2 for high-quality image synthesis.
    </div>
  </div>
  <div class="citation">
    <div class="image"><img src="./assets/cross-domain.png"></div>
    <div class="comment">
      <a href="https://arxiv.org/pdf/2104.06820.pdf" target="_blank">
        U. Ojha, Y. Li, J. Lu, AA. Efros, YJ. Lee, E. Shechtman, and R. Zhang.
        Few-shot Image Generation via Cross-domain Correspondence
        CVPR, 2021.</a><br>
      <b>Comment:</b>
      Proposed the cross-domain consistency as a regularization to maintain the diversity.
    </div>
  </div> -->

</div>
<!-- === Reference Section Ends === -->


</body>
</html>
